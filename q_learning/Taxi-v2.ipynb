{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import training\n",
    "import benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = 'Taxi-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction with the environment\n",
    "Explore the environment to understand its dynamics, observations and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Discrete(500)\n",
      "Observation space sample: 383\n",
      "Action space: Discrete(6)\n",
      "Action space sample: 5\n",
      "State: 293\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(ENV)\n",
    "state = test_env.reset()\n",
    "print(f'Observation space: {test_env.observation_space}')\n",
    "print(f'Observation space sample: {test_env.observation_space.sample()}')\n",
    "\n",
    "print(f'Action space: {test_env.action_space}')\n",
    "print(f'Action space sample: {test_env.action_space.sample()}')\n",
    "\n",
    "print(f'State: {state}')\n",
    "test_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play random action and see how it affects the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: 1\tNew state: 193\tReward: -1\tDone:False\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n"
     ]
    }
   ],
   "source": [
    "random_action = test_env.action_space.sample()\n",
    "new_state, reward, done, _ = test_env.step(random_action)\n",
    "\n",
    "print(f'Action: {random_action}\\tNew state: {new_state}\\tReward: {reward}\\tDone:{done}')\n",
    "test_env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the environment using Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000: New best score! 8.300000190734863\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV)\n",
    "\n",
    "best_score, q_table = training.fit(env, episodes=10_000)\n",
    "rewards = benchmark.play_episodes(env, q_table, episodes=100)\n",
    "\n",
    "mean_reward = rewards.mean().item()\n",
    "print(f'Mean reward: {mean_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.play_episodes(env, q_table, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save q-table to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(q_table, 'Taxi-v2_saved/Taxi-v2-qtable.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Table plot\n",
    "Plotting the Q-Table shows clearly that the actions 4 an 5 which correspond to pickup and dropoff are of the most value.\n",
    "This is to be expected since performing those actions correctly will return a reward of +10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(q_table, cmap='hot', aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
