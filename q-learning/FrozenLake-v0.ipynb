{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env,\n",
    "          episodes=10_000,\n",
    "          validate_n=1000,\n",
    "          validation_episodes=100,\n",
    "          learning_rate=0.1, learning_rate_min=0.005, learning_rate_decay=0.9995,\n",
    "          epsilon=1.0, epsilon_decay=0.999, epsilon_min=0.01,\n",
    "          discount_factor=0.99,\n",
    "          verbose=True):\n",
    "\n",
    "    q_table = torch.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    best_q_table = q_table.clone()\n",
    "    best_score = 0.0\n",
    "\n",
    "    for ep in range(1, episodes+1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Determine action via exploration or explotation according to random value\n",
    "            if torch.rand(1).item() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = torch.argmax(q_table[state]).item()\n",
    "\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            # update q table using bellman's equation\n",
    "            target_value = torch.max(q_table[new_state])\n",
    "            q_table[state, action] += learning_rate*(reward + discount_factor*target_value - q_table[state, action])   \n",
    "\n",
    "            state = new_state\n",
    "\n",
    "        # update exploration probability\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay  \n",
    "\n",
    "        # update learning rate\n",
    "        if learning_rate > learning_rate_min:\n",
    "            learning_rate *= learning_rate_decay  \n",
    "\n",
    "        if ep % validate_n == 0:\n",
    "            rewards = play_episodes(validation_episodes, env, q_table)\n",
    "            mean_reward = rewards.mean().item()\n",
    "\n",
    "            if mean_reward > best_score:\n",
    "                best_score = mean_reward\n",
    "                best_q_table = q_table.clone()\n",
    "                if verbose:\n",
    "                    print(f'Episode {ep}: New best score! {best_score}')\n",
    "                    \n",
    "    return best_score, best_q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(count, env, q_table):\n",
    "    rewards = torch.zeros((count,))\n",
    "\n",
    "    for ep in range(count):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = torch.argmax(q_table[state]).item()\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "        rewards[ep] = total_reward\n",
    "    \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000: New best score! 0.6899999976158142\n",
      "Episode 2000: New best score! 0.7300000190734863\n",
      "Episode 4000: New best score! 0.800000011920929\n",
      "Episode 11000: New best score! 0.8100000023841858\n",
      "Best score: 0.8100000023841858\n",
      "tensor([[0.5403, 0.5306, 0.5309, 0.5274],\n",
      "        [0.3141, 0.3425, 0.3712, 0.4981],\n",
      "        [0.4120, 0.4040, 0.4000, 0.4726],\n",
      "        [0.2701, 0.2963, 0.2484, 0.4605],\n",
      "        [0.5562, 0.3551, 0.4010, 0.3824],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2775, 0.1341, 0.3503, 0.1148],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3946, 0.4370, 0.4133, 0.5869],\n",
      "        [0.3992, 0.6318, 0.4255, 0.3740],\n",
      "        [0.5968, 0.5278, 0.4253, 0.2575],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4428, 0.4626, 0.7384, 0.5445],\n",
      "        [0.7324, 0.8690, 0.7785, 0.7684],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "score, q_table = train(env, episodes=30_000, learning_rate=0.5)\n",
    "\n",
    "print(f'Best score: {score}')\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7360000014305115"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes = 1000\n",
    "rewards = play_episodes(episodes, env, q_table)\n",
    "rewards.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8X8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000: New best score! 0.8500000238418579\n",
      "Episode 9000: New best score! 0.8700000047683716\n",
      "Episode 11000: New best score! 0.9399999976158142\n",
      "Best score: 0.9399999976158142\n",
      "tensor([[3.9947e-01, 4.0201e-01, 4.0182e-01, 4.0481e-01],\n",
      "        [4.0455e-01, 4.1206e-01, 4.1668e-01, 4.1174e-01],\n",
      "        [4.1953e-01, 4.2392e-01, 4.3389e-01, 4.2666e-01],\n",
      "        [4.4213e-01, 4.4340e-01, 4.5310e-01, 4.4257e-01],\n",
      "        [4.5683e-01, 4.6264e-01, 4.7376e-01, 4.6359e-01],\n",
      "        [4.8375e-01, 4.8542e-01, 4.9662e-01, 4.8533e-01],\n",
      "        [5.1099e-01, 5.1175e-01, 5.1334e-01, 5.0875e-01],\n",
      "        [5.1879e-01, 5.1811e-01, 5.2026e-01, 5.1879e-01],\n",
      "        [3.8526e-01, 3.8283e-01, 3.8710e-01, 4.0185e-01],\n",
      "        [3.8425e-01, 3.8625e-01, 3.9429e-01, 4.1038e-01],\n",
      "        [3.7138e-01, 3.6023e-01, 3.9192e-01, 4.2459e-01],\n",
      "        [2.4708e-01, 2.6435e-01, 2.6520e-01, 4.4358e-01],\n",
      "        [4.3376e-01, 4.2349e-01, 4.4269e-01, 4.6643e-01],\n",
      "        [4.6684e-01, 4.7454e-01, 4.9354e-01, 4.7730e-01],\n",
      "        [5.2035e-01, 5.2146e-01, 5.2271e-01, 5.1643e-01],\n",
      "        [5.3337e-01, 5.4016e-01, 5.3358e-01, 5.2767e-01],\n",
      "        [2.7927e-01, 2.8095e-01, 3.0855e-01, 3.7403e-01],\n",
      "        [3.4852e-01, 2.3159e-01, 2.3021e-01, 2.9386e-01],\n",
      "        [1.5371e-01, 7.6333e-02, 1.7760e-01, 2.7318e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1668e-01, 1.9491e-01, 3.9872e-01, 2.8243e-01],\n",
      "        [2.8503e-01, 3.3513e-01, 4.0308e-01, 4.7561e-01],\n",
      "        [5.1055e-01, 5.2625e-01, 5.3654e-01, 5.2096e-01],\n",
      "        [5.6007e-01, 5.6068e-01, 5.7179e-01, 5.5299e-01],\n",
      "        [1.1155e-01, 6.3125e-02, 2.0879e-02, 3.3160e-01],\n",
      "        [6.2929e-02, 1.2364e-01, 9.9754e-02, 2.9593e-01],\n",
      "        [2.2438e-01, 4.3460e-02, 7.6579e-02, 2.6604e-02],\n",
      "        [3.4948e-02, 1.5708e-01, 3.0847e-03, 5.5830e-02],\n",
      "        [2.4767e-01, 3.5321e-02, 1.0504e-01, 7.5049e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.6758e-01, 3.7029e-01, 5.3883e-01, 3.7234e-01],\n",
      "        [5.9474e-01, 6.0440e-01, 6.1710e-01, 5.7938e-01],\n",
      "        [5.9395e-03, 2.4132e-04, 3.4759e-04, 9.9361e-02],\n",
      "        [0.0000e+00, 3.4044e-07, 1.0830e-03, 1.6079e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 7.9578e-04, 1.0703e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.1051e-02, 7.1971e-03, 3.0695e-02, 1.6505e-01],\n",
      "        [4.1541e-02, 2.4724e-01, 1.2070e-01, 9.5343e-02],\n",
      "        [3.2461e-01, 2.6092e-01, 3.1884e-01, 4.8356e-01],\n",
      "        [6.3913e-01, 6.4509e-01, 6.8367e-01, 5.9745e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.8212e-03],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 6.1119e-03],\n",
      "        [0.0000e+00, 2.0086e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [2.0982e-02, 5.0893e-03, 8.7575e-02, 4.1835e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [4.3067e-01, 4.7018e-01, 7.7814e-01, 5.1388e-01],\n",
      "        [3.0060e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 1.6200e-04, 4.1818e-04, 2.7679e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [6.7221e-01, 5.8720e-01, 8.7767e-01, 5.8758e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.3843e-04],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0949e-03],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0612e-01],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')\n",
    "score, q_table = train(env, episodes=30_000, learning_rate=0.5)\n",
    "\n",
    "print(f'Best score: {score}')\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8410000205039978"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episodes = 1000\n",
    "rewards = play_episodes(episodes, env, q_table)\n",
    "rewards.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
